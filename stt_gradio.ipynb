{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stt_gradio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPeZ+ea0EZUALNXiU83IUTl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsnam95/class2022Spring/blob/main/stt_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load model and tokenizer\n",
        "!pip install transformers\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "!pip install datasets\n",
        "from datasets import load_dataset\n",
        "\n",
        "# load dummy dataset and read soundfiles\n",
        "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "\n",
        "# tokenize\n",
        "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
        "\n",
        "# retrieve logits\n",
        "logits = model(input_values).logits\n",
        "\n",
        "# take argmax and decode\n",
        "import torch\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)"
      ],
      "metadata": {
        "id": "cCjYRKPqcq4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# figure out how the model works\n",
        "processor\n",
        "s = ds[0][\"audio\"][\"array\"]\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(s)\n",
        "transcription[0]"
      ],
      "metadata": {
        "id": "Hz8V01Kmgn_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa\n",
        "def stt (audio):\n",
        "\n",
        "  # normalize signal from gradio mic\n",
        "  import numpy as np\n",
        "  sig = audio[1]/np.max(np.abs(audio[1]))\n",
        "\n",
        "  # resample 44100Hz(from gradio mic) to 16000Hz(trained on this model)\n",
        "  import librosa\n",
        "  sig = librosa.resample(sig, orig_sr=audio[0], target_sr=16000)\n",
        "\n",
        "  # load model and tokenizer\n",
        "  processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "  model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "  # tokenize\n",
        "  input_values = processor(sig, return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
        "\n",
        "  # retrieve logits\n",
        "  logits = model(input_values).logits\n",
        "\n",
        "  # take argmax and decode\n",
        "  import torch\n",
        "  predicted_ids = torch.argmax(logits, dim=-1)\n",
        "  transcription = processor.batch_decode(predicted_ids)\n",
        "  return transcription[0]"
      ],
      "metadata": {
        "id": "Eh_44HuIcq7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "import gradio as gr\n",
        "iface = gr.Interface(stt, inputs = [\"mic\"], outputs = [\"text\"])\n",
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "id": "yO8QuZM_cq-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}